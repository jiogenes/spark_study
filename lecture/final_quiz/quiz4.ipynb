{"cells":[{"cell_type":"markdown","metadata":{"id":"t2GwT7nYG83Z"},"source":["### Data\n","Downloaded from LMS\n","\n","### Question 1.\n","\n","Summary of data set: Each line of input file \"heart.csv\" represents a health record such as age, sex and chest pain type related to the heart disease. The column named \"target\" refers to the presence of heart disease in the patient. It is an integer valued 0 = disease, 1 = no disease.\n","\n","Columns:\n","* age (numeric)\n","* sex (numeric)\n","* cp: chest pain type (4 integer values from 0 to 3)\n","* trestbps: resting blood pressure (numeric)\n","* chol: serum cholestoral in mg/dl (numeric)\n","* fbs: fasting blood sugar > 120 mg/dl (numeric)\n","* restecg: resting electrocardiographic results (integer values 0,1,2)\n","* thalach: maximum heart rate achieved (numeric)\n","* exang: exercise induced angina (1 = yes; 0 = no)\n","* oldpeak: ST depression induced by exercise relative to rest (numeric)\n","* slope: the slope of the peak exercise ST segment (valued 0, 1 or 2)\n","* ca: number of major vessels (0-3) colored by flourosopy (integer)\n","* thal: Thalium, a radioactive tracer injected during a stress test ranged from 0 to 3\n","\n","Our goal is to learn a logistic regression model that predicts the presence of disease.\n","\n","### 문제 1.\n","\n","데이터셋 요약: 파일 \"heart_disease_uci.csv\"의 각 라인은 심장병과 관련된 환자의 건강지수를 나타낸다. 나이, 성별, 가슴통증 타입등과 같은 정보를 담고 있다. \"target\"이란 이름의 컬럼은 환자가 심장병이 있는 것으로 진단되었는지 여부를 나타내며 0이면 양성, 1이면 음성 판정이다.\n","\n","문제: 주어진 데이터셋을 이용하여 로지스틱 회귀 모델을 학습하여 심장병 유무를 예측하라."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"J7nSD9DrG83c"},"outputs":[{"name":"stdout","output_type":"stream","text":["22/12/01 16:06:39 WARN Utils: Your hostname, orange resolves to a loopback address: 127.0.1.1; using 166.104.246.51 instead (on interface enp15s0)\n","22/12/01 16:06:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"]},{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["22/12/01 16:06:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["# create spark session\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"my app\").master(\"local\").getOrCreate()\n","\n","# get context from the session\n","sc = spark.sparkContext"]},{"cell_type":"markdown","metadata":{"id":"L3JU1V7KG83d"},"source":["(1) 우선 schema를 잘 정의하여 위 칼럼 설명에 나온 대로 float 타입과 integer 타입을 정의하여 데이터프레임 data을 생성하시오 (10점)\n","\n","힌트: column 이름은 age, sex, dataset, cp, trestbps, chol, fbs, restecg, thalch, exang, oldpeak, \tslope, ca, thal, target\n","\n","힌트: string type은 sex, dataset, cp, fbs, restecg, exang, slope, thal이고 integer type은 ca, target이며 나머지는 float type으로 정의하시오."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"8NWi-Db8G83e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+------+---------+---------------+--------+-----+-----+--------------+------+-----+-------+-----------+---+------------+------+\n","| age|   sex|  dataset|             cp|trestbps| chol|  fbs|       restecg|thalch|exang|oldpeak|      slope| ca|        thal|target|\n","+----+------+---------+---------------+--------+-----+-----+--------------+------+-----+-------+-----------+---+------------+------+\n","|63.0|  Male|Cleveland| typical angina|   145.0|233.0| TRUE|lv hypertrophy| 150.0|FALSE|    2.3|downsloping|  0|fixed defect|     0|\n","|37.0|  Male|Cleveland|    non-anginal|   130.0|250.0|FALSE|        normal| 187.0|FALSE|    3.5|downsloping|  0|      normal|     0|\n","|41.0|Female|Cleveland|atypical angina|   130.0|204.0|FALSE|lv hypertrophy| 172.0|FALSE|    1.4|  upsloping|  0|      normal|     0|\n","|56.0|  Male|Cleveland|atypical angina|   120.0|236.0|FALSE|        normal| 178.0|FALSE|    0.8|  upsloping|  0|      normal|     0|\n","|57.0|Female|Cleveland|   asymptomatic|   120.0|354.0|FALSE|        normal| 163.0| TRUE|    0.6|  upsloping|  0|      normal|     0|\n","+----+------+---------+---------------+--------+-----+-----+--------------+------+-----+-------+-----------+---+------------+------+\n","only showing top 5 rows\n","\n"]},{"data":{"text/plain":["676"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["from pyspark.sql.types import *\n","\n","schema = StructType([\n","    StructField('age', FloatType(), True),\n","    StructField('sex', StringType(), True),\n","    StructField('dataset', StringType(), True),\n","    StructField('cp', StringType(), True),\n","    StructField('trestbps', FloatType(), True),\n","    StructField('chol', FloatType(), True),\n","    StructField('fbs', StringType(), True),\n","    StructField('restecg', StringType(), True),\n","    StructField('thalch', FloatType(), True),\n","    StructField('exang', StringType(), True),\n","    StructField('oldpeak', FloatType(), True),\n","    StructField('slope', StringType(), True),\n","    StructField('ca', IntegerType(), True),\n","    StructField('thal', StringType(), True),\n","    StructField('target', IntegerType(), True),\n","])\n","\n","data = spark.read.format('csv')\\\n","                 .option('header', 'true')\\\n","                 .option(\"quote\", \"\\\"\")\\\n","                 .option(\"escape\", \"\\\"\")\\\n","                 .option(\"multiLine\", \"true\")\\\n","                 .schema(schema)\\\n","                 .load('../../data/heart_disease_uci.csv')\n","\n","data.show(5)\n","data.count()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- age: float (nullable = true)\n"," |-- sex: string (nullable = true)\n"," |-- dataset: string (nullable = true)\n"," |-- cp: string (nullable = true)\n"," |-- trestbps: float (nullable = true)\n"," |-- chol: float (nullable = true)\n"," |-- fbs: string (nullable = true)\n"," |-- restecg: string (nullable = true)\n"," |-- thalch: float (nullable = true)\n"," |-- exang: string (nullable = true)\n"," |-- oldpeak: float (nullable = true)\n"," |-- slope: string (nullable = true)\n"," |-- ca: integer (nullable = true)\n"," |-- thal: string (nullable = true)\n"," |-- target: integer (nullable = true)\n","\n"]}],"source":["data.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"FI1IOV-4dJel"},"source":["(1-1) 우선 Null을 하나라도 포함한 row를 모두 제거하시오."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"-QfMSanpdFKQ"},"outputs":[{"data":{"text/plain":["216"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["data = data.dropna()\n","data.count()"]},{"cell_type":"markdown","metadata":{"id":"uhFpebPyG83e"},"source":["(2) integer type의 컬럼들은 값이 category 이기 때문에 logistic regression을 사용할때 이를 다차원 실수벡터로 변환하여 많이 분석한다. 예를 들면 color 라는 컬럼이 있고 0:red, 1:blue, 3:black이라면 0값을 갖는다고 color가 낮은 것이 아니고 3값을 갖는다고 color가 높은 것이 아니다. 이런 경우 3차원 벡터로 확장하여 red인 경우 (0, 0, 1), blue인경우 (0, 1, 0), black인경우 (1, 0, 0)과 같이 실수벡터로 확장, 치환하여 분석을 많이 한다. 이러한 방법을 one hot encoding이라고 한다. \n","\n","string type의 컬럼들을 모두 이와 같이 one hot encoding 하여 새로운 데이터프레임을 생성해 data에 덮어쓰시오. one hot encoding에 앞서서 string indexing을 해야 함을 잊지 않도록. (15점)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"fy9B9RTfQERD"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+-------------+--------+---------+-------------+-----------+-----------+----------+\n","|sex_index|dataset_index|cp_index|fbs_index|restecg_index|exang_index|slope_index|thal_index|\n","+---------+-------------+--------+---------+-------------+-----------+-----------+----------+\n","|      0.0|          0.0|     3.0|      1.0|          1.0|        0.0|        2.0|       2.0|\n","|      0.0|          0.0|     1.0|      0.0|          0.0|        0.0|        2.0|       0.0|\n","|      1.0|          0.0|     2.0|      0.0|          1.0|        0.0|        0.0|       0.0|\n","|      0.0|          0.0|     2.0|      0.0|          0.0|        0.0|        0.0|       0.0|\n","|      1.0|          0.0|     0.0|      0.0|          0.0|        1.0|        0.0|       0.0|\n","+---------+-------------+--------+---------+-------------+-----------+-----------+----------+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.ml.feature import StringIndexer\n","string_cols = ['sex', 'dataset', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n","string_indexer = StringIndexer(inputCols=string_cols, outputCols=[i + '_index' for i in string_cols])\n","data = string_indexer.fit(data).transform(data)\n","data.select(*[i + '_index' for i in string_cols]).show(5)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n","|      sex_ohe|  dataset_ohe|       cp_ohe|      fbs_ohe|  restecg_ohe|    exang_ohe|    slope_ohe|     thal_ohe|       ca_ohe|\n","+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n","|(1,[0],[1.0])|(2,[0],[1.0])|    (3,[],[])|    (1,[],[])|(2,[1],[1.0])|(1,[0],[1.0])|    (2,[],[])|    (2,[],[])|(3,[0],[1.0])|\n","|(1,[0],[1.0])|(2,[0],[1.0])|(3,[1],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(1,[0],[1.0])|    (2,[],[])|(2,[0],[1.0])|(3,[0],[1.0])|\n","|    (1,[],[])|(2,[0],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])|(2,[1],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|(3,[0],[1.0])|\n","|(1,[0],[1.0])|(2,[0],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|(3,[0],[1.0])|\n","|    (1,[],[])|(2,[0],[1.0])|(3,[0],[1.0])|(1,[0],[1.0])|(2,[0],[1.0])|    (1,[],[])|(2,[0],[1.0])|(2,[0],[1.0])|(3,[0],[1.0])|\n","+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.ml.feature import OneHotEncoder\n","categorical_cols = ['sex', 'dataset', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'ca']\n","onehotencoder = OneHotEncoder(inputCols=[i + '_index' for i in string_cols] + ['ca'], outputCols=[i + '_ohe' for i in categorical_cols])\n","data = onehotencoder.fit(data).transform(data)\n","data.select(*[i + '_ohe' for i in categorical_cols]).show(5)"]},{"cell_type":"markdown","metadata":{"id":"ckRpjCvqG83f"},"source":["(3) 다음은 분석에 사용할 컬럼을 feature vector로 변환하여 dataframe에 추가하기 위해, 우선 분석에 사용할 컬럼 이름의 리스트를 뽑아내는 일이다. 다음 셀에 분석에 사용할 컬럼 이름의 리스트를 cols에 저장하는 코드를 작성하시오. 위 StringIndexer와 OneHotEncoder에서 입력으로 사용했던 컬럼들은 제거하고. (10점)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"-b0Oab_MG83f"},"outputs":[{"data":{"text/plain":["['age',\n"," 'trestbps',\n"," 'chol',\n"," 'thalch',\n"," 'oldpeak',\n"," 'sex_ohe',\n"," 'dataset_ohe',\n"," 'cp_ohe',\n"," 'fbs_ohe',\n"," 'restecg_ohe',\n"," 'exang_ohe',\n"," 'slope_ohe',\n"," 'thal_ohe',\n"," 'ca_ohe']"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["cols = data.columns\n","cols.remove('target')\n","for i in categorical_cols:\n","    cols.remove(i)\n","    if i != 'ca':\n","        cols.remove(i+'_index')\n","cols"]},{"cell_type":"markdown","metadata":{"id":"1LgM6Qy-G83g"},"source":["(4) 이제 VectorAssembler를 이용해서 cols의 컬럼들을 한데 합쳐 feature vector를 만들어 data에 다시 넣도록 하자. 새로 추가될 feature vector 의 컬럼의 이름은 \"features\"라고 하자. (10점)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"QGmT00ewG83g"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+\n","|            features|\n","+--------------------+\n","|(22,[0,1,2,3,4,5,...|\n","|(22,[0,1,2,3,4,5,...|\n","|(22,[0,1,2,3,4,6,...|\n","|[56.0,120.0,236.0...|\n","|(22,[0,1,2,3,4,6,...|\n","+--------------------+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.ml.feature import VectorAssembler\n","\n","vector_assembler = VectorAssembler(inputCols = cols, outputCol='features')\n","data = vector_assembler.transform(data)\n","data.select('features').show(5)"]},{"cell_type":"markdown","metadata":{"id":"2adU4UuEG83h"},"source":["(5) 데이터프레임 data를 8대2의 비율로 나누어 각각 train, test란 이름의 데이터프레임으로 쪼갠다. pyspark.sql.DataFrame.randomSplit 함수를 사용하고 seed는 편의상 1을 주기로 하자. (10점)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"Fgzgcj-YG83h"},"outputs":[{"data":{"text/plain":["(175, 41)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["train, test = data.randomSplit([0.8, 0.2], 1)\n","train.count(), test.count()"]},{"cell_type":"markdown","metadata":{"id":"X_wqQXg5G83h"},"source":["(6) 이제 LogisticRegression을 이용하여 학습을 시작한다. 학습에는 데이터프레임 train의 칼럼 features를 사용한다. 그리고 LogisticRegression 모델의 학습된 결과는 model 이란 이름의 변수명을 사용하여 저장한다. 또한 모델 생성자 생성 시, 예측결과는 default대로 prediction 컬럼에 쓰도록 설정하고 iteration은 10회만 실시하도록 설정한다. LogisticRegression 모델의 생성자와 모델이 다름을 주의할 것. (10점)"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"wwQ2lxhbG83h"},"outputs":[],"source":["from pyspark.ml.classification import LogisticRegression\n","\n","lr = LogisticRegression(labelCol='target', maxIter=10)\n","model = lr.fit(train)"]},{"cell_type":"markdown","metadata":{"id":"x0tTEGJ5G83h"},"source":["(7) model을 이용해 데이터프레임 test의 각 row에 대해 심장병 유무를 예측한다. 예측결과가 추가된 데이터프레임은 predict_test란 이름으로 저장한다. (10점)"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"nkIK37ixG83h"},"outputs":[],"source":["predict_test = model.transform(test)"]},{"cell_type":"markdown","metadata":{"id":"nqyeRKFoG83i"},"source":["(8) predict_test의 결과에 대해 정확도와 거짓양성(false positive)와 거짓음성(false negative)의 비율을 계산한다. (25점)\n","\n","* 정확도는 전체 데이터 중 예측결과 prediction와 참 값 target이 같은 비율을 말한다.\n","* 거짓양성은 예측결과가 양성 (즉, 병이 있음. 다시 말해 prediction = 0)인 데이터 중 틀린 비율 (즉, 병이 없음; target = 1)을 말한다.\n","* 거짓음성은 예측결과가 음성 (즉, 병이 없음; prediction = 1)인 데이터 중 틀린 비율 (즉, 병이 있음; target = 0)을 말한다."]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy : 0.7317073170731707\n","precision : 0.5454545454545454\n","recall : 0.5\n"]}],"source":["real_pos_cnt = predict_test.rdd.map(lambda row: 1 if row['target'] == 1 else 0).reduce(lambda x, y: x + y)\n","pred_pos_cnt = predict_test.rdd.map(lambda row: 1 if row['prediction'] == 1 else 0).reduce(lambda x, y: x + y)\n","TP = predict_test.rdd.map(lambda row: 1 if row['target'] == 1 and row['target'] == row['prediction'] else 0).reduce(lambda x, y: x + y)\n","correct_cnt = predict_test.rdd.map(lambda row: 1 if row['target'] == row['prediction'] else 0).reduce(lambda x, y: x + y)\n","print(f'accuracy : {correct_cnt / predict_test.count()}')\n","print(f'precision : {TP/pred_pos_cnt}')\n","print(f'recall : {TP/real_pos_cnt}')"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["sc.stop()"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 ('spark')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"2d3673617709ceda87ac6d055fc19541640288a4bee74fd36c0e58c49d867a56"}}},"nbformat":4,"nbformat_minor":0}
